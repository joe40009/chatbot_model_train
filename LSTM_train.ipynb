{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim import models\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import logging\n",
    "from keras import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Bidirectional,LSTM,Dense,Embedding,Dropout,Activation,Softmax\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "import pymysql\n",
    "import numpy\n",
    "from opencc import OpenCC\n",
    "import jieba.posseg as jp\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE, SVMSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sql = \"select quest_nm_adj, class_nm from textdb.fact_cls_qa_kw where quest_kw is not null\" #SQL指令拉資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pymysql.connect(host='172.16.56.101', port=31996, user='root', passwd='password', db='textdb' )\n",
    "cursor = db.cursor()\n",
    "cursor.execute(sql)\n",
    "kw_results = cursor.fetchall() #從MYSQL拉出資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = pd.read_csv('data/fact_cls_qa_kw.csv', encoding='big5') #從CSV拉資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_results = [] #整理CSV資料\n",
    "for i in range(len(csv_data)):\n",
    "    kw_results.append([\n",
    "    list(csv_data['quest_nm_adj'])[i],\n",
    "    list(csv_data['class_nm'])[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['帳單周期看得到嗎?', '帳務查詢'],\n",
       " ['我要請你幫我查一隻電話號碼帳單多少錢', '帳務查詢'],\n",
       " ['想詢問這期帳單', '帳務查詢'],\n",
       " ['我想要查詢我電話目前帳單地址', '帳務查詢'],\n",
       " ['我的帳單每個月是多少錢', '帳務查詢']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_results[0:5] #資料展示前5筆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'合約查詢': 10, '帳務查詢': 10, '魔速方塊1.0': 10, '魔速方塊2.0': 10}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "class_list_orgn = []\n",
    "for raw in kw_results:\n",
    "    rsl = raw[1].split(',')\n",
    "    for rs in rsl:\n",
    "        class_list_orgn.append(rs)\n",
    "unique, counts = numpy.unique(class_list_orgn, return_counts=True)\n",
    "dict(zip(unique, counts)) #統計各class筆數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'合約查詢': 0, '帳務查詢': 1, '魔速方塊1.0': 2, '魔速方塊2.0': 3}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "label_dic = {}\n",
    "for label_raw in unique:\n",
    "#     print(label_raw[0])\n",
    "    label_dic[label_raw] = i\n",
    "    i += 1\n",
    "class_list = []\n",
    "for classes in class_list_orgn:\n",
    "    class_list.append(label_dic[classes])\n",
    "    \n",
    "label_dic #把CLASS名稱轉成數字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['帳單周期看得到嗎?',\n",
       " '我要請你幫我查一隻電話號碼帳單多少錢',\n",
       " '想詢問這期帳單',\n",
       " '我想要查詢我電話目前帳單地址',\n",
       " '我的帳單每個月是多少錢',\n",
       " '我想請問一下你們的帳單截止日是幾號?',\n",
       " '我辦理NP後還有一筆帳單還未繳',\n",
       " '我這次要繳多少錢',\n",
       " '請問我的帳單是幾號繳費',\n",
       " '我想要查我手機的帳單',\n",
       " '我現在的資費方案是?',\n",
       " '我的方案是啥',\n",
       " '請告訴我當初辦理的專案',\n",
       " '目前資費是什麼',\n",
       " '我忘了當初申辦的資費了',\n",
       " '我想要解約NP去別家',\n",
       " '怎麼辦理解約',\n",
       " '不想用這個門號了，我要解約',\n",
       " '收訊太爛了，我不想用了啦，立刻給我解約',\n",
       " '離譜的訊號品質，給我解了吧',\n",
       " '魔速方塊1.0和Wi-Fi通話有什麼不同',\n",
       " '魔速方塊1.0設備規格支援哪種頻段',\n",
       " '如果用戶非亞太本網，魔速方塊1.0也支援嗎',\n",
       " '要去哪裡申請魔速方塊1.0呢',\n",
       " '如果有企業用戶要申請魔速方塊，請問要去哪邊申辦',\n",
       " '亞太用戶都可以申裝廆速方塊1.0嗎',\n",
       " '回到家不會安裝魔速方塊怎麼辦',\n",
       " '魔速方塊有問題或需要維修怎麼辦',\n",
       " '如果用戶搬家，是否需要歸還魔速方塊',\n",
       " '我不小心弄壞魔速方塊1.0了，要賠錢嗎',\n",
       " '魔速方塊2.0發射功率為多少',\n",
       " '魔速方塊2.0在一般情況下耗電率如何',\n",
       " '每天、一個月消耗電量以及電費預估?',\n",
       " '魔速方塊2.0是否可以增強網路的數據下載與上傳速率',\n",
       " '怎麼申請魔速方塊2.0?需要費用嗎?',\n",
       " '魔速方塊2.0功能為何',\n",
       " '電器產品擺在魔速方塊2.0附近是否會影響訊號轉換後的功率',\n",
       " '魔速方塊2.0是否會跟任何WiFi訊號互相干擾',\n",
       " '有沒有環境或條件的外在影響，會造成魔速方塊2.0的訊號功效不如預期?',\n",
       " '魔速方塊2.0對高樓層有限制嗎']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list = []\n",
    "for raw in kw_results:\n",
    "    data_list.append(raw[0])\n",
    "data_list #訓練句子資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'一',\n",
       " '一些',\n",
       " '一來',\n",
       " '一切',\n",
       " '一旦',\n",
       " '一起響應環保愛地球',\n",
       " '上',\n",
       " '下',\n",
       " '不',\n",
       " '不但',\n",
       " '不僅',\n",
       " '不僅僅',\n",
       " '不光',\n",
       " '不只',\n",
       " '不單',\n",
       " '不外乎',\n",
       " '不好意思',\n",
       " '不如',\n",
       " '不料',\n",
       " '不是',\n",
       " '不會',\n",
       " '不然',\n",
       " '不盡',\n",
       " '不盡然',\n",
       " '不管',\n",
       " '不至於',\n",
       " '不論',\n",
       " '不過',\n",
       " '且',\n",
       " '並',\n",
       " '並不',\n",
       " '並且',\n",
       " '並非',\n",
       " '乃',\n",
       " '乃至',\n",
       " '之',\n",
       " '之所以',\n",
       " '也',\n",
       " '了',\n",
       " '些',\n",
       " '亞太電信推出無紙化簡訊及e-mail電子帳單服務',\n",
       " '亦',\n",
       " '人',\n",
       " '人們',\n",
       " '什麼',\n",
       " '什麼樣',\n",
       " '什麼的',\n",
       " '今',\n",
       " '介於',\n",
       " '仍',\n",
       " '仍舊',\n",
       " '他',\n",
       " '他人',\n",
       " '他們',\n",
       " '以',\n",
       " '以上',\n",
       " '以來',\n",
       " '以便',\n",
       " '以免',\n",
       " '以及',\n",
       " '以為',\n",
       " '以至',\n",
       " '以致',\n",
       " '任何',\n",
       " '似的',\n",
       " '但',\n",
       " '但是',\n",
       " '何',\n",
       " '何以',\n",
       " '何時',\n",
       " '何況',\n",
       " '何處',\n",
       " '作為',\n",
       " '你',\n",
       " '你們',\n",
       " '使',\n",
       " '來',\n",
       " '來自',\n",
       " '來說',\n",
       " '例如',\n",
       " '依據',\n",
       " '依照',\n",
       " '便於',\n",
       " '個',\n",
       " '個人',\n",
       " '個別',\n",
       " '們',\n",
       " '倘若',\n",
       " '值此',\n",
       " '假如',\n",
       " '儘管',\n",
       " '儘管如此',\n",
       " '先生',\n",
       " '光是',\n",
       " '兒',\n",
       " '全部',\n",
       " '全體',\n",
       " '兩者',\n",
       " '其',\n",
       " '其中',\n",
       " '其他',\n",
       " '其它',\n",
       " '其次',\n",
       " '其餘',\n",
       " '再',\n",
       " '再則',\n",
       " '再有',\n",
       " '再見',\n",
       " '凡',\n",
       " '凡是',\n",
       " '出來',\n",
       " '出於',\n",
       " '分別',\n",
       " '別',\n",
       " '別人',\n",
       " '別的',\n",
       " '別處',\n",
       " '別說',\n",
       " '到',\n",
       " '則',\n",
       " '前者',\n",
       " '加之',\n",
       " '加以',\n",
       " '即',\n",
       " '即使',\n",
       " '即便',\n",
       " '卻',\n",
       " '去',\n",
       " '又',\n",
       " '又及',\n",
       " '及',\n",
       " '及至',\n",
       " '反之',\n",
       " '反而',\n",
       " '受到',\n",
       " '另',\n",
       " '另一方面',\n",
       " '另外',\n",
       " '只',\n",
       " '只因',\n",
       " '只怕',\n",
       " '只是',\n",
       " '只有',\n",
       " '只消',\n",
       " '只要',\n",
       " '只限',\n",
       " '只限於',\n",
       " '只需',\n",
       " '可',\n",
       " '可以',\n",
       " '可是',\n",
       " '可見',\n",
       " '各',\n",
       " '各位',\n",
       " '各自',\n",
       " '同',\n",
       " '同時',\n",
       " '向',\n",
       " '向著',\n",
       " '否則',\n",
       " '吧',\n",
       " '呵呵',\n",
       " '和',\n",
       " '咱',\n",
       " '咱們',\n",
       " '哇',\n",
       " '哪',\n",
       " '哪些',\n",
       " '哪個',\n",
       " '哪兒',\n",
       " '哪怕',\n",
       " '唯有',\n",
       " '啥',\n",
       " '啦',\n",
       " '喲',\n",
       " '嗡',\n",
       " '嘛',\n",
       " '嘻嘻',\n",
       " '嘿嘿',\n",
       " '因',\n",
       " '因之',\n",
       " '因此',\n",
       " '因為',\n",
       " '因而',\n",
       " '固然',\n",
       " '在',\n",
       " '在於',\n",
       " '基於',\n",
       " '多少',\n",
       " '多會',\n",
       " '多麼',\n",
       " '大家',\n",
       " '她',\n",
       " '她們',\n",
       " '好',\n",
       " '如',\n",
       " '如上',\n",
       " '如下',\n",
       " '如何',\n",
       " '如同下',\n",
       " '如是',\n",
       " '如果',\n",
       " '如果說',\n",
       " '如此',\n",
       " '如若',\n",
       " '它',\n",
       " '它們',\n",
       " '客戶結束交談',\n",
       " '寧可',\n",
       " '對待',\n",
       " '對方',\n",
       " '對於',\n",
       " '對比',\n",
       " '小',\n",
       " '小姐',\n",
       " '就',\n",
       " '就是',\n",
       " '就是說',\n",
       " '就算',\n",
       " '就要',\n",
       " '已',\n",
       " '幾',\n",
       " '彼時',\n",
       " '彼此',\n",
       " '往',\n",
       " '很',\n",
       " '很高興為您服務',\n",
       " '後',\n",
       " '後者',\n",
       " '得',\n",
       " '得了',\n",
       " '從',\n",
       " '從此',\n",
       " '從而',\n",
       " '怎',\n",
       " '怎樣',\n",
       " '怎麼',\n",
       " '怎麼樣',\n",
       " '怎麼辦',\n",
       " '您',\n",
       " '您好',\n",
       " '感謝您使用亞太文字客服服務',\n",
       " '憑',\n",
       " '憑藉',\n",
       " '我',\n",
       " '我們',\n",
       " '我們將繼續為您服務',\n",
       " '或',\n",
       " '或是',\n",
       " '或者',\n",
       " '或者說',\n",
       " '截至',\n",
       " '所',\n",
       " '所以',\n",
       " '所在',\n",
       " '所有',\n",
       " '才是',\n",
       " '才能',\n",
       " '打',\n",
       " '把',\n",
       " '拿',\n",
       " '按照',\n",
       " '接著',\n",
       " '據',\n",
       " '據此',\n",
       " '故而',\n",
       " '於',\n",
       " '於是',\n",
       " '既',\n",
       " '既往',\n",
       " '既是',\n",
       " '既然',\n",
       " '早安',\n",
       " '是',\n",
       " '是否',\n",
       " '曾',\n",
       " '替代',\n",
       " '最',\n",
       " '會',\n",
       " '有',\n",
       " '有些',\n",
       " '有時',\n",
       " '有的',\n",
       " '有關',\n",
       " '朝著',\n",
       " '未接獲您的回應已超過',\n",
       " '本人',\n",
       " '本地',\n",
       " '本著',\n",
       " '本身',\n",
       " '果然',\n",
       " '某',\n",
       " '某些',\n",
       " '某個',\n",
       " '某某',\n",
       " '根據',\n",
       " '格里斯',\n",
       " '歡迎您向服務人員提出申請需求',\n",
       " '正值',\n",
       " '正如',\n",
       " '正巧',\n",
       " '正是',\n",
       " '此',\n",
       " '此地',\n",
       " '此外',\n",
       " '此時',\n",
       " '此次',\n",
       " '此處',\n",
       " '此間',\n",
       " '毋寧',\n",
       " '每',\n",
       " '每當',\n",
       " '比',\n",
       " '比如',\n",
       " '沿',\n",
       " '沿著',\n",
       " '況且',\n",
       " '為',\n",
       " '為了',\n",
       " '為什麼',\n",
       " '為何',\n",
       " '為止',\n",
       " '為此',\n",
       " '為著',\n",
       " '無',\n",
       " '無論',\n",
       " '然後',\n",
       " '然而',\n",
       " '照著',\n",
       " '爾',\n",
       " '甚而',\n",
       " '甚至',\n",
       " '甚至於',\n",
       " '用',\n",
       " '用來',\n",
       " '由',\n",
       " '由於',\n",
       " '由此',\n",
       " '當',\n",
       " '當地',\n",
       " '當然',\n",
       " '的',\n",
       " '的確',\n",
       " '的話',\n",
       " '直到',\n",
       " '看',\n",
       " '等等',\n",
       " '簡言之',\n",
       " '系統即將結束對話',\n",
       " '約',\n",
       " '給',\n",
       " '經過',\n",
       " '總之',\n",
       " '繼而',\n",
       " '而',\n",
       " '而且',\n",
       " '而外',\n",
       " '而已',\n",
       " '而後',\n",
       " '而是',\n",
       " '自',\n",
       " '自己',\n",
       " '自從',\n",
       " '自身',\n",
       " '至',\n",
       " '至今',\n",
       " '至於',\n",
       " '致',\n",
       " '與',\n",
       " '與其',\n",
       " '與否',\n",
       " '若',\n",
       " '若您正在回覆中',\n",
       " '若是',\n",
       " '若非',\n",
       " '萬一',\n",
       " '著',\n",
       " '處在',\n",
       " '被',\n",
       " '要',\n",
       " '要不',\n",
       " '要不然',\n",
       " '要是',\n",
       " '要麼',\n",
       " '許多',\n",
       " '該',\n",
       " '說來',\n",
       " '誰',\n",
       " '誰人',\n",
       " '請',\n",
       " '請問',\n",
       " '請您見諒',\n",
       " '請於30秒內隨意輸入任何文字、數字或符號',\n",
       " '諸',\n",
       " '諸位',\n",
       " '諸如',\n",
       " '謝謝',\n",
       " '譬如',\n",
       " '讓',\n",
       " '貝貝',\n",
       " '賴以',\n",
       " '起',\n",
       " '趁',\n",
       " '趁著',\n",
       " '距',\n",
       " '跟',\n",
       " '較之',\n",
       " '逐步',\n",
       " '這',\n",
       " '這些',\n",
       " '這個',\n",
       " '這兒',\n",
       " '這會',\n",
       " '這樣',\n",
       " '這般',\n",
       " '這裡',\n",
       " '這邊',\n",
       " '這麼',\n",
       " '造成您不便',\n",
       " '連同',\n",
       " '連帶',\n",
       " '進而',\n",
       " '遵循',\n",
       " '遵照',\n",
       " '還',\n",
       " '還是',\n",
       " '還有',\n",
       " '還要',\n",
       " '那',\n",
       " '那些',\n",
       " '那個',\n",
       " '那兒',\n",
       " '那時',\n",
       " '那樣',\n",
       " '那般',\n",
       " '那裡',\n",
       " '那邊',\n",
       " '那麼',\n",
       " '都',\n",
       " '針對',\n",
       " '鑑於',\n",
       " '開外',\n",
       " '關於',\n",
       " '除了',\n",
       " '除外',\n",
       " '除此',\n",
       " '除非',\n",
       " '隨',\n",
       " '隨後',\n",
       " '隨時',\n",
       " '隨著',\n",
       " '雖',\n",
       " '雖然',\n",
       " '雖說',\n",
       " '非但',\n",
       " '靠',\n",
       " '首先',\n",
       " '麼',\n",
       " '\\ufeff客戶名稱'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc= OpenCC('s2twp')\n",
    "words_file = './stopwords.txt' #文本檔案位置\n",
    "stopwords_set = set()                                           #创建set集合\n",
    "with open(words_file, 'r', encoding = 'utf-8') as f:            #打开文件\n",
    "    for line in f.readlines():                                  #一行一行读取\n",
    "        word = line.strip()                                     #去回车\n",
    "        if len(word) > 0:                                       #有文本，则添加到words_set中\n",
    "            stopwords_set.add(cc.convert(word))\n",
    "stopwords_set #停用字詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.095 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "jieba.load_userdict(\"./jieba_dict.txt\") #讀取自建辭庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 W2V model\n",
    "def train_word2vec(sentences,save_path):\n",
    "    sentences_seg = []\n",
    "    for i in sentences:\n",
    "        sen_proc = []\n",
    "        try:\n",
    "            for word in jieba.cut(i):\n",
    "                sen_proc.append(word)\n",
    "            sentences_seg.append(sen_proc)\n",
    "        except:\n",
    "             continue                \n",
    "    print(\"start w2v train\") \n",
    "    model = Word2Vec(sentences_seg,\n",
    "                size=100,  # 词向量维度\n",
    "                min_count=5,  # 词频阈值\n",
    "                window=5)  # 窗口大小    \n",
    "    model.save(save_path)\n",
    "    print(\"w2v train complete\")\n",
    "    return model, sentences_seg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start w2v train\n",
      "w2v train complete\n"
     ]
    }
   ],
   "source": [
    "# train W2V\n",
    "model, sentences_seg =  train_word2vec(data_list, './LSTM_model/w2v_model/word2vec.model.bin')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W2V編號與權重\n",
    "def generate_id2wec(word2vec_model):\n",
    "    gensim_dict = Dictionary()\n",
    "    gensim_dict.doc2bow(model.wv.vocab.keys(), allow_update=True)\n",
    "    w2id = {v: k + 1 for k, v in gensim_dict.items()}  # 词语的索引，从1开始编号\n",
    "    w2vec = {word: model[word] for word in w2id.keys()}  # 词语的词向量\n",
    "    n_vocabs = len(w2id) + 1\n",
    "    embedding_weights = np.zeros((n_vocabs, 100))\n",
    "    for w, index in w2id.items():  # 从索引为1的词语开始，用词向量填充矩阵\n",
    "        embedding_weights[index, :] = w2vec[w]\n",
    "    return w2id,embedding_weights\n",
    "# 句子轉成W2V編號\n",
    "def text_to_array(w2index, senlist):  # 文本转为索引数字模式\n",
    "    sentences_array = []\n",
    "    for sen in senlist:\n",
    "        new_sen = [ w2index.get(word,0) for word in sen]   # 单词转索引数字\n",
    "        sentences_array.append(new_sen)\n",
    "    return np.array(sentences_array)\n",
    "# 準備訓練資料\n",
    "def prepare_data(w2id,sentences,labels,max_len=200):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(sentences,labels, test_size=0.2)\n",
    "    X_train = text_to_array(w2id, X_train)\n",
    "    X_val = text_to_array(w2id, X_val)\n",
    "    X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "    X_val = pad_sequences(X_val, maxlen=max_len)\n",
    "    return np.array(X_train), np_utils.to_categorical(y_train) ,np.array(X_val), np_utils.to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 產生W2V編號與權重\n",
    "w2id,embedding_weights = generate_id2wec(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_trian, x_val , y_val = prepare_data(w2id,sentences_seg,class_list,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 LSTM model\n",
    "class Sentiment:\n",
    "    def __init__(self,w2id,embedding_weights,Embedding_dim,maxlen,labels_category, save_model_path='./LSTM_model/LSTM_model/sentiment.h5'):\n",
    "        self.Embedding_dim = Embedding_dim\n",
    "        self.embedding_weights = embedding_weights\n",
    "        self.vocab = w2id\n",
    "        self.labels_category = labels_category\n",
    "        self.maxlen = maxlen\n",
    "        self.model = self.build_model()\n",
    "        self.smp = save_model_path\n",
    "        \n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        #input dim(140,100)\n",
    "        model.add(Embedding(output_dim = self.Embedding_dim,\n",
    "                           input_dim=len(self.vocab)+1,\n",
    "                           weights=[self.embedding_weights],\n",
    "                           input_length=self.maxlen))\n",
    "        model.add(Bidirectional(LSTM(50),merge_mode='concat'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(self.labels_category))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                     optimizer='adam', \n",
    "                     metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    def train(self,X_train, y_train,X_test, y_test,n_epoch=5 ):\n",
    "        self.model.fit(X_train, y_train, batch_size=32, epochs=n_epoch,\n",
    "                      validation_data=(X_test, y_test))\n",
    "        self.model.save(self.smp)   \n",
    "        \n",
    "    def predict(self,model_path,new_sen):\n",
    "        model = self.model\n",
    "        model.load_weights(model_path)\n",
    "        new_sen_list = jieba.lcut(new_sen)\n",
    "        sen2id =[ self.vocab.get(word,0) for word in new_sen_list]\n",
    "        sen_input = pad_sequences([sen2id], maxlen=self.maxlen)\n",
    "        res = model.predict(sen_input)[0]\n",
    "        return np.argmax(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 訓練 LSTM\n",
    "# Sentiment(w2id,embedding_weights, 词向量维度, 句子長度, class數目, model儲存位置')\n",
    "senti = Sentiment(w2id,embedding_weights,100,200,4, './LSTM_model/LSTM_model/sentiment.h5')\n",
    "# senti.train(x_train,y_trian, x_val ,y_val, Epoch次數)\n",
    "senti.train(x_train,y_trian, x_val ,y_val,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取儲存LSTM model\n",
    "model = load_model('./LSTM_model/LSTM_model/sentiment.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試精準度與混淆矩陣\n",
    "prc = []\n",
    "for prw in data_list:\n",
    "    new_sen_list = jieba.lcut(prw)\n",
    "    sen2id = [w2id.get(word,0) for word in new_sen_list]\n",
    "    sen_input = pad_sequences([sen2id], maxlen=200)\n",
    "    res = model.predict(sen_input)[0]\n",
    "    prc.append(np.argmax(res))\n",
    "ps = 0\n",
    "for pi in range(len(class_list)):\n",
    "    if class_list[pi] == prc[pi]:\n",
    "        ps += 1\n",
    "prs = ps / len(class_list)\n",
    "prs\n",
    "confusion = confusion_matrix(class_list, prc, labels=[num for num in range(len(label_dic))]) #與實際答案做混淆矩陣\n",
    "plt.figure()\n",
    "print(label_dic)\n",
    "print('acc: %.2f' % (prs*100) + '%')\n",
    "sns.heatmap(confusion,annot=True, xticklabels=[num for num in range(len(label_dic))], yticklabels=[num for num in range(len(label_dic))], annot_kws={\"size\": 20}) #畫出熱度圖\n",
    "plt.show(sns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SMOTE資料增強\n",
    "\n",
    "# SMOTE\n",
    "oversampler=SMOTE(random_state=45)\n",
    "x_resampled, y_resampled = oversampler.fit_sample(x_train, y_trian)\n",
    "senti = Sentiment(w2id,embedding_weights,100,200,4, './LSTM_model/LSTM_model/sentiment_s1.h5')\n",
    "senti.train(x_resampled,y_resampled, x_val ,y_val,5)\n",
    "# SMOTE kind=borderline-1\n",
    "oversampler=BorderlineSMOTE(random_state=45, kind='borderline-1')\n",
    "x_resampled, y_resampled = oversampler.fit_sample(x_train, y_trian)\n",
    "senti = Sentiment(w2id,embedding_weights,100,200,4, './LSTM_model/LSTM_model/sentiment_s2.h5')\n",
    "senti.train(x_resampled,y_resampled, x_val ,y_val,5)\n",
    "# SMOTE kind=borderline-2\n",
    "oversampler=BorderlineSMOTE(random_state=45, kind='borderline-2')\n",
    "x_resampled, y_resampled = oversampler.fit_sample(x_train, y_trian)\n",
    "senti = Sentiment(w2id,embedding_weights,100,200,4, './LSTM_model/LSTM_model/sentiment_s3.h5')\n",
    "senti.train(x_resampled,y_resampled, x_val ,y_val,5)\n",
    "# SMOTEENN\n",
    "oversampler=SMOTEENN(random_state=45)\n",
    "x_resampled, y_resampled = oversampler.fit_sample(x_train, y_trian)\n",
    "senti = Sentiment(w2id,embedding_weights,100,200,4, './LSTM_model/LSTM_model/sentiment_s4.h5')\n",
    "senti.train(x_resampled,y_resampled, x_val ,y_val,5)\n",
    "# SMOTETomek\n",
    "oversampler=SMOTETomek(random_state=45)\n",
    "x_resampled, y_resampled = oversampler.fit_sample(x_train, y_trian)\n",
    "senti = Sentiment(w2id,embedding_weights,100,200,4, './LSTM_model/LSTM_model/sentiment_s5.h5')\n",
    "senti.train(x_resampled,y_resampled, x_val ,y_val,5)\n",
    "# ADASYN\n",
    "try:\n",
    "    oversampler=ADASYN(random_state=45)\n",
    "    x_resampled, y_resampled = oversampler.fit_sample(x_train, y_trian)\n",
    "    senti = Sentiment(w2id,embedding_weights,100,200,4, './LSTM_model/LSTM_model/sentiment_s6.h5')\n",
    "    senti.train(x_resampled,y_resampled, x_val ,y_val,5)\n",
    "except ValueError:\n",
    "    print('ADASYN: No samples will be generated with the provided ratio settings.')\n",
    "# SVMSMOTE\n",
    "oversampler=SVMSMOTE(random_state=45)\n",
    "x_resampled, y_resampled = oversampler.fit_sample(x_train, y_trian)\n",
    "senti = Sentiment(w2id,embedding_weights,100,200,4, './LSTM_model/LSTM_model/sentiment_s7.h5')\n",
    "senti.train(x_resampled,y_resampled, x_val ,y_val,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#讀取SMOTE資料增強後儲存LSTM model\n",
    "model1 = load_model('./LSTM_model/LSTM_model/sentiment_s1.h5')\n",
    "model2 = load_model('./LSTM_model/LSTM_model/sentiment_s2.h5')\n",
    "model3 = load_model('./LSTM_model/LSTM_model/sentiment_s3.h5')\n",
    "model4 = load_model('./LSTM_model/LSTM_model/sentiment_s4.h5')\n",
    "model5 = load_model('./LSTM_model/LSTM_model/sentiment_s5.h5')\n",
    "# model6 = load_model('./LSTM_model/LSTM_model/sentiment_s6.h5')\n",
    "model7 = load_model('./LSTM_model/LSTM_model/sentiment_s7.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#測試SMOTE資料增強後精準度與混淆矩陣\n",
    "\n",
    "print(label_dic)\n",
    "plt.figure(figsize=(15,20))\n",
    "plt.subplot(4,2,1)\n",
    "plt.title('origin predict: ' + str(round(prs*100, 2)) + ' %')\n",
    "sns.heatmap(confusion,annot=True, xticklabels=[num for num in range(len(label_dic))], yticklabels=[num for num in range(len(label_dic))], annot_kws={\"size\": 20}) #畫出熱度圖\n",
    "\n",
    "# print('==============SMOTE=====================================')\n",
    "prc = []\n",
    "for prw in data_list:\n",
    "    new_sen_list = jieba.lcut(prw)\n",
    "    sen2id = [w2id.get(word,0) for word in new_sen_list]\n",
    "    sen_input = pad_sequences([sen2id], maxlen=200)\n",
    "    res = model1.predict(sen_input)[0]\n",
    "    prc.append(np.argmax(res))\n",
    "ps = 0\n",
    "for pi in range(len(class_list)):\n",
    "    if class_list[pi] == prc[pi]:\n",
    "        ps += 1\n",
    "prs = ps / len(class_list)\n",
    "prs\n",
    "confusion = confusion_matrix(class_list, prc, labels=[num for num in range(len(label_dic))]) #與實際答案做混淆矩陣\n",
    "plt.subplot(4,2,2)\n",
    "plt.title('SMOTE predict: ' + str(round(prs*100, 2)) + ' %')\n",
    "sns.heatmap(confusion,annot=True, xticklabels=[num for num in range(len(label_dic))], yticklabels=[num for num in range(len(label_dic))], annot_kws={\"size\": 20}) #畫出熱度圖\n",
    "\n",
    "# print('==============SMOTE kind=borderline-1=====================================')\n",
    "prc = []\n",
    "for prw in data_list:\n",
    "    new_sen_list = jieba.lcut(prw)\n",
    "    sen2id = [w2id.get(word,0) for word in new_sen_list]\n",
    "    sen_input = pad_sequences([sen2id], maxlen=200)\n",
    "    res = model2.predict(sen_input)[0]\n",
    "    prc.append(np.argmax(res))\n",
    "ps = 0\n",
    "for pi in range(len(class_list)):\n",
    "    if class_list[pi] == prc[pi]:\n",
    "        ps += 1\n",
    "prs = ps / len(class_list)\n",
    "prs\n",
    "confusion = confusion_matrix(class_list, prc, labels=[num for num in range(len(label_dic))]) #與實際答案做混淆矩陣\n",
    "plt.subplot(4,2,3)\n",
    "plt.title('SMOTE kind=borderline-1 predict: ' + str(round(prs*100, 2)) + ' %')\n",
    "sns.heatmap(confusion,annot=True, xticklabels=[num for num in range(len(label_dic))], yticklabels=[num for num in range(len(label_dic))], annot_kws={\"size\": 20}) #畫出熱度圖\n",
    "\n",
    "# print('==============SMOTE kind=borderline-2=====================================')\n",
    "prc = []\n",
    "for prw in data_list:\n",
    "    new_sen_list = jieba.lcut(prw)\n",
    "    sen2id = [w2id.get(word,0) for word in new_sen_list]\n",
    "    sen_input = pad_sequences([sen2id], maxlen=200)\n",
    "    res = model3.predict(sen_input)[0]\n",
    "    prc.append(np.argmax(res))\n",
    "ps = 0\n",
    "for pi in range(len(class_list)):\n",
    "    if class_list[pi] == prc[pi]:\n",
    "        ps += 1\n",
    "prs = ps / len(class_list)\n",
    "prs\n",
    "confusion = confusion_matrix(class_list, prc, labels=[num for num in range(len(label_dic))]) #與實際答案做混淆矩陣\n",
    "plt.subplot(4,2,4)\n",
    "plt.title('SMOTE kind=borderline-2 predict: ' + str(round(prs*100, 2)) + ' %')\n",
    "sns.heatmap(confusion,annot=True, xticklabels=[num for num in range(len(label_dic))], yticklabels=[num for num in range(len(label_dic))], annot_kws={\"size\": 20}) #畫出熱度圖\n",
    "\n",
    "# print('==============SMOTEENN=====================================')\n",
    "prc = []\n",
    "for prw in data_list:\n",
    "    new_sen_list = jieba.lcut(prw)\n",
    "    sen2id = [w2id.get(word,0) for word in new_sen_list]\n",
    "    sen_input = pad_sequences([sen2id], maxlen=200)\n",
    "    res = model4.predict(sen_input)[0]\n",
    "    prc.append(np.argmax(res))\n",
    "ps = 0\n",
    "for pi in range(len(class_list)):\n",
    "    if class_list[pi] == prc[pi]:\n",
    "        ps += 1\n",
    "prs = ps / len(class_list)\n",
    "prs\n",
    "confusion = confusion_matrix(class_list, prc, labels=[num for num in range(len(label_dic))]) #與實際答案做混淆矩陣\n",
    "plt.subplot(4,2,5)\n",
    "plt.title('SMOTEENN predict: ' + str(round(prs*100, 2)) + ' %')\n",
    "sns.heatmap(confusion,annot=True, xticklabels=[num for num in range(len(label_dic))], yticklabels=[num for num in range(len(label_dic))], annot_kws={\"size\": 20}) #畫出熱度圖\n",
    "\n",
    "# print('==============SMOTETomek=====================================')\n",
    "prc = []\n",
    "for prw in data_list:\n",
    "    new_sen_list = jieba.lcut(prw)\n",
    "    sen2id = [w2id.get(word,0) for word in new_sen_list]\n",
    "    sen_input = pad_sequences([sen2id], maxlen=200)\n",
    "    res = model5.predict(sen_input)[0]\n",
    "    prc.append(np.argmax(res))\n",
    "ps = 0\n",
    "for pi in range(len(class_list)):\n",
    "    if class_list[pi] == prc[pi]:\n",
    "        ps += 1\n",
    "prs = ps / len(class_list)\n",
    "prs\n",
    "confusion = confusion_matrix(class_list, prc, labels=[num for num in range(len(label_dic))]) #與實際答案做混淆矩陣\n",
    "plt.subplot(4,2,6)\n",
    "plt.title('SMOTETomek predict: ' + str(round(prs*100, 2)) + ' %')\n",
    "sns.heatmap(confusion,annot=True, xticklabels=[num for num in range(len(label_dic))], yticklabels=[num for num in range(len(label_dic))], annot_kws={\"size\": 20}) #畫出熱度圖\n",
    "\n",
    "# print('==============ADASYN=====================================')\n",
    "# prc = []\n",
    "# for prw in data_list:\n",
    "#     new_sen_list = jieba.lcut(prw)\n",
    "#     sen2id = [w2id.get(word,0) for word in new_sen_list]\n",
    "#     sen_input = pad_sequences([sen2id], maxlen=200)\n",
    "#     res = model6.predict(sen_input)[0]\n",
    "#     prc.append(np.argmax(res))\n",
    "# ps = 0\n",
    "# for pi in range(len(class_list)):\n",
    "#     if class_list[pi] == prc[pi]:\n",
    "#         ps += 1\n",
    "# prs = ps / len(class_list)\n",
    "# prs\n",
    "# confusion = confusion_matrix(class_list, prc, labels=[num for num in range(len(label_dic))]) #與實際答案做混淆矩陣\n",
    "# plt.subplot(4,2,7)\n",
    "# plt.title('ADASYN predict: ' + str(round(prs*100, 2)) + ' %')\n",
    "# sns.heatmap(confusion,annot=True, xticklabels=[num for num in range(len(label_dic))], yticklabels=[num for num in range(len(label_dic))], annot_kws={\"size\": 20}) #畫出熱度圖\n",
    "\n",
    "# print('==============SVMSMOTE=====================================')\n",
    "prc = []\n",
    "for prw in data_list:\n",
    "    new_sen_list = jieba.lcut(prw)\n",
    "    sen2id = [w2id.get(word,0) for word in new_sen_list]\n",
    "    sen_input = pad_sequences([sen2id], maxlen=200)\n",
    "    res = model7.predict(sen_input)[0]\n",
    "    prc.append(np.argmax(res))\n",
    "ps = 0\n",
    "for pi in range(len(class_list)):\n",
    "    if class_list[pi] == prc[pi]:\n",
    "        ps += 1\n",
    "prs = ps / len(class_list)\n",
    "prs\n",
    "confusion = confusion_matrix(class_list, prc, labels=[num for num in range(len(label_dic))]) #與實際答案做混淆矩陣\n",
    "plt.subplot(4,2,8)\n",
    "plt.title('SVMSMOTE predict: ' + str(round(prs*100, 2)) + ' %')\n",
    "sns.heatmap(confusion,annot=True, xticklabels=[num for num in range(len(label_dic))], yticklabels=[num for num in range(len(label_dic))], annot_kws={\"size\": 20}) #畫出熱度圖\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model 句子測試\n",
    "\n",
    "new_sen = '解約金是多少'\n",
    "\n",
    "new_sen_list = jieba.lcut(new_sen)\n",
    "sen2id = [w2id.get(word,0) for word in new_sen_list]\n",
    "sen_input = pad_sequences([sen2id], maxlen=200)\n",
    "res = model.predict(sen_input)[0]\n",
    "print('origin: ' + list(label_dic.keys())[list(label_dic.values()).index(np.argmax(res))])\n",
    "res = model1.predict(sen_input)[0]\n",
    "print('SMOTE: ' + list(label_dic.keys())[list(label_dic.values()).index(np.argmax(res))])\n",
    "res = model2.predict(sen_input)[0]\n",
    "print('SMOTE kind=borderline-1: ' + list(label_dic.keys())[list(label_dic.values()).index(np.argmax(res))])\n",
    "res = model3.predict(sen_input)[0]\n",
    "print('SMOTE kind=borderline-2: ' + list(label_dic.keys())[list(label_dic.values()).index(np.argmax(res))])\n",
    "res = model4.predict(sen_input)[0]\n",
    "print('SMOTEENN: ' + list(label_dic.keys())[list(label_dic.values()).index(np.argmax(res))])\n",
    "res = model5.predict(sen_input)[0]\n",
    "print('SMOTETomek: ' + list(label_dic.keys())[list(label_dic.values()).index(np.argmax(res))])\n",
    "# res = model6.predict(sen_input)[0]\n",
    "# print('ADASYN: ' +  list(label_dic.keys())[list(label_dic.values()).index(np.argmax(res))])\n",
    "res = model7.predict(sen_input)[0]\n",
    "print('SVMSMOTE: ' + list(label_dic.keys())[list(label_dic.values()).index(np.argmax(res))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'sentiment.h5'[:-3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
